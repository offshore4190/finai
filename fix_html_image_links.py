"""
ä¿®å¤HTMLæ–‡ä»¶ä¸­çš„å›¾ç‰‡é“¾æ¥
å°†ç»å¯¹è·¯å¾„æˆ–SEC URLé‡å†™ä¸ºç›¸å¯¹è·¯å¾„

ä½¿ç”¨æ–¹æ³•ï¼š
  python fix_html_image_links.py                      # ä¿®å¤æ‰€æœ‰HTMLæ–‡ä»¶
  python fix_html_image_links.py --sample 50          # éšæœºæŠ½æ ·50ä¸ªæ–‡ä»¶
  python fix_html_image_links.py --exchange NASDAQ    # åªä¿®å¤NASDAQ
  python fix_html_image_links.py --dry-run            # é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹
"""

import argparse
import re
import sys
from pathlib import Path
from collections import defaultdict
from typing import Dict, List
import random

from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
from config.settings import settings
import structlog

logger = structlog.get_logger()


class HTMLImageLinkFixer:
    """HTMLå›¾ç‰‡é“¾æ¥ä¿®å¤å™¨"""
    
    def __init__(self, dry_run: bool = False):
        self.storage_root = Path(settings.storage_root)
        self.dry_run = dry_run
        self.engine = create_engine(settings.database_url)
        
        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'total_files': 0,
            'files_fixed': 0,
            'links_fixed': 0,
            'errors': 0
        }
        
    def get_image_mapping(self, html_path: Path) -> Dict[str, str]:
        """
        è·å–è¯¥HTMLæ–‡ä»¶å¯¹åº”çš„å›¾ç‰‡æ˜ å°„
        ä»æ•°æ®åº“æŸ¥è¯¢è¯¥filingçš„æ‰€æœ‰å›¾ç‰‡ï¼Œå»ºç«‹URLåˆ°æœ¬åœ°ç›¸å¯¹è·¯å¾„çš„æ˜ å°„
        
        Returns:
            {åŸå§‹URL: ç›¸å¯¹è·¯å¾„}
        """
        # ä»HTMLè·¯å¾„æå–ä¿¡æ¯
        # æ ¼å¼: NASDAQ/AAPL/2023/aapl-20230930.htm
        relative_path = str(html_path.relative_to(self.storage_root))
        
        with self.engine.connect() as conn:
            # æŸ¥æ‰¾è¯¥HTMLæ–‡ä»¶å¯¹åº”çš„artifact
            result = conn.execute(text("""
                SELECT a.id, a.filing_id
                FROM artifacts a
                WHERE a.local_path = :html_path
                  AND a.artifact_type = 'html'
                LIMIT 1
            """), {'html_path': relative_path})
            
            row = result.fetchone()
            if not row:
                logger.warning("html_artifact_not_found", path=relative_path)
                return {}
            
            artifact_id, filing_id = row
            
            # æŸ¥è¯¢è¯¥filingçš„æ‰€æœ‰å›¾ç‰‡
            result = conn.execute(text("""
                SELECT url, local_path, filename
                FROM artifacts
                WHERE filing_id = :filing_id
                  AND artifact_type = 'image'
                  AND status IN ('downloaded', 'skipped')
            """), {'filing_id': filing_id})
            
            mapping = {}
            for row in result:
                url, local_path, filename = row
                
                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                # HTMLåœ¨: NYSE/AB/2024/ab-20231231.html
                # å›¾ç‰‡åœ¨: NYSE/AB/2024/ab-20231231_image-001.jpg
                # ç›¸å¯¹è·¯å¾„: ./ab-20231231_image-001.jpg
                if local_path:
                    image_path = Path(local_path)
                    relative_to_html = f"./{image_path.name}"
                    mapping[url] = relative_to_html
            
            logger.debug(
                "image_mapping_built",
                html_path=relative_path,
                filing_id=filing_id,
                image_count=len(mapping)
            )
            
            return mapping
    
    def fix_html_file(self, html_path: Path) -> Dict:
        """ä¿®å¤å•ä¸ªHTMLæ–‡ä»¶"""
        result = {
            'path': str(html_path.relative_to(self.storage_root)),
            'fixed': False,
            'links_fixed': 0,
            'original_links': [],
            'new_links': []
        }
        
        try:
            # è¯»å–HTMLæ–‡ä»¶
            with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
                original_content = f.read()
            
            # è·å–å›¾ç‰‡æ˜ å°„
            image_mapping = self.get_image_mapping(html_path)
            
            if not image_mapping:
                logger.debug("no_images_for_html", path=result['path'])
                return result
            
            # è§£æHTML
            soup = BeautifulSoup(original_content, 'lxml')
            img_tags = soup.find_all('img')
            
            modified = False
            
            for img in img_tags:
                src = img.get('src', '')
                
                if not src:
                    continue
                
                # å°è¯•åŒ¹é…åŸå§‹URL
                new_src = None
                
                # ç›´æ¥åŒ¹é…
                if src in image_mapping:
                    new_src = image_mapping[src]
                else:
                    # å°è¯•å»æ‰file:///å‰ç¼€åŒ¹é…
                    if src.startswith('file:///'):
                        # file:///private/tmp/filings/NYSE/AB/2024/ab-20231231_g2.jpg
                        # æå–æ–‡ä»¶åéƒ¨åˆ†
                        filename = Path(src).name
                        
                        # åœ¨æ˜ å°„ä¸­æŸ¥æ‰¾åŒ…å«æ­¤æ–‡ä»¶åçš„URL
                        for url, relative_path in image_mapping.items():
                            if filename in url or filename in relative_path:
                                new_src = relative_path
                                break
                    
                    # å°è¯•åŒ¹é…URLä¸­çš„æ–‡ä»¶å
                    if not new_src:
                        src_filename = Path(src).name
                        for url, relative_path in image_mapping.items():
                            url_filename = Path(url).name
                            if src_filename == url_filename:
                                new_src = relative_path
                                break
                
                # å¦‚æœæ‰¾åˆ°äº†æ–°çš„ç›¸å¯¹è·¯å¾„ï¼Œè¿›è¡Œæ›¿æ¢
                if new_src and new_src != src:
                    result['original_links'].append(src)
                    result['new_links'].append(new_src)
                    img['src'] = new_src
                    modified = True
                    result['links_fixed'] += 1
            
            if modified and not self.dry_run:
                # ä¿å­˜ä¿®æ”¹åçš„HTML
                new_content = str(soup)
                
                # å¤‡ä»½åŸæ–‡ä»¶
                backup_path = html_path.with_suffix('.html.bak')
                if not backup_path.exists():
                    with open(backup_path, 'w', encoding='utf-8') as f:
                        f.write(original_content)
                
                # å†™å…¥æ–°å†…å®¹
                with open(html_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                
                result['fixed'] = True
                self.stats['files_fixed'] += 1
                self.stats['links_fixed'] += result['links_fixed']
                
                logger.info(
                    "html_fixed",
                    path=result['path'],
                    links_fixed=result['links_fixed']
                )
            elif modified:
                result['fixed'] = True  # Would be fixed
                self.stats['links_fixed'] += result['links_fixed']
            
            return result
            
        except Exception as e:
            result['error'] = str(e)
            self.stats['errors'] += 1
            logger.error("fix_failed", path=result['path'], error=str(e))
            return result
    
    def scan_html_files(self, exchange: str = None, sample_size: int = None) -> List[Path]:
        """æ‰«æHTMLæ–‡ä»¶"""
        print(f"ğŸ“ æ‰«æHTMLæ–‡ä»¶...")
        
        if not self.storage_root.exists():
            print(f"âš ï¸  å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {self.storage_root}")
            return []
        
        html_files = []
        
        for exchange_dir in self.storage_root.iterdir():
            if not exchange_dir.is_dir():
                continue
            
            if exchange and exchange_dir.name != exchange:
                continue
            
            for company_dir in exchange_dir.iterdir():
                if not company_dir.is_dir():
                    continue
                
                for html_file in company_dir.rglob('*.html'):
                    # è·³è¿‡å¤‡ä»½æ–‡ä»¶
                    if html_file.suffix == '.bak':
                        continue
                    html_files.append(html_file)
                for htm_file in company_dir.rglob('*.htm'):
                    html_files.append(htm_file)
        
        if sample_size and len(html_files) > sample_size:
            html_files = random.sample(html_files, sample_size)
        
        print(f"âœ… æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        return html_files
    
    def run(self, exchange: str = None, sample_size: int = None):
        """è¿è¡Œä¿®å¤"""
        mode_str = "é¢„è§ˆæ¨¡å¼" if self.dry_run else "ä¿®å¤æ¨¡å¼"
        print("\n" + "=" * 100)
        print(f"ğŸ”§ HTMLå›¾ç‰‡é“¾æ¥ä¿®å¤å·¥å…· ({mode_str})")
        print("=" * 100 + "\n")
        
        html_files = self.scan_html_files(exchange, sample_size)
        
        if not html_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶")
            return
        
        self.stats['total_files'] = len(html_files)
        
        print(f"\nå¼€å§‹å¤„ç† {len(html_files)} ä¸ªHTMLæ–‡ä»¶...\n")
        
        fixed_files = []
        
        for i, html_path in enumerate(html_files, 1):
            if i % 100 == 0:
                print(f"  è¿›åº¦: {i}/{len(html_files)}")
            
            result = self.fix_html_file(html_path)
            
            if result['fixed']:
                fixed_files.append(result)
        
        # æ‰“å°æŠ¥å‘Š
        self.print_report(fixed_files)
    
    def print_report(self, fixed_files: List[Dict]):
        """æ‰“å°æŠ¥å‘Š"""
        print("\n" + "=" * 100)
        print("ğŸ“Š ä¿®å¤æŠ¥å‘Š")
        print("=" * 100 + "\n")
        
        mode_str = " (é¢„è§ˆæ¨¡å¼ï¼Œæœªå®é™…ä¿®æ”¹)" if self.dry_run else ""
        
        print(f"ã€æ€»ä½“ç»Ÿè®¡{mode_str}ã€‘")
        print(f"  å¤„ç†æ–‡ä»¶æ•°: {self.stats['total_files']:,}")
        print(f"  ä¿®å¤çš„æ–‡ä»¶: {self.stats['files_fixed']:,}")
        print(f"  ä¿®å¤çš„é“¾æ¥: {self.stats['links_fixed']:,}")
        print(f"  å¤„ç†é”™è¯¯: {self.stats['errors']:,}")
        
        if fixed_files:
            print(f"\nâœ… æˆåŠŸä¿®å¤ {len(fixed_files)} ä¸ªæ–‡ä»¶")
            
            print("\nã€ä¿®å¤çš„æ–‡ä»¶ã€‘ï¼ˆå‰20ä¸ªï¼‰")
            for i, result in enumerate(fixed_files[:20], 1):
                print(f"\n{i}. æ–‡ä»¶: {result['path']}")
                print(f"   ä¿®å¤é“¾æ¥æ•°: {result['links_fixed']}")
                
                if result['original_links'] and result['new_links']:
                    print(f"   ç¤ºä¾‹:")
                    for orig, new in zip(result['original_links'][:2], result['new_links'][:2]):
                        print(f"     {orig[:80]}...")
                        print(f"     â†’ {new}")
            
            if len(fixed_files) > 20:
                print(f"\n... è¿˜æœ‰ {len(fixed_files) - 20} ä¸ªæ–‡ä»¶æœªåˆ—å‡º")
        else:
            print("\nâœ… æ‰€æœ‰HTMLæ–‡ä»¶çš„å›¾ç‰‡é“¾æ¥éƒ½å·²ç»æ˜¯æ­£ç¡®çš„ç›¸å¯¹è·¯å¾„ï¼")
        
        if self.stats['errors'] > 0:
            print(f"\nâš ï¸  å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿ {self.stats['errors']} ä¸ªé”™è¯¯")
        
        print("\n" + "=" * 100)
        
        if self.dry_run:
            print("â„¹ï¸  è¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæ²¡æœ‰å®é™…ä¿®æ”¹æ–‡ä»¶ã€‚")
            print("   è¦å®é™…ä¿®å¤ï¼Œè¯·å»æ‰ --dry-run å‚æ•°ã€‚")
        elif self.stats['files_fixed'] > 0:
            print(f"ğŸ‰ ä¿®å¤å®Œæˆï¼å·²ä¿®å¤ {self.stats['files_fixed']} ä¸ªæ–‡ä»¶ã€‚")
            print("   åŸæ–‡ä»¶å·²å¤‡ä»½ä¸º .html.bak")
        else:
            print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½æ­£å¸¸ï¼Œæ— éœ€ä¿®å¤ã€‚")
        
        print("=" * 100 + "\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä¿®å¤HTMLæ–‡ä»¶ä¸­çš„å›¾ç‰‡é“¾æ¥',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # é¢„è§ˆæ¨¡å¼ï¼ˆä¸å®é™…ä¿®æ”¹ï¼‰
  python fix_html_image_links.py --dry-run --sample 10
  
  # ä¿®å¤æ‰€æœ‰HTMLæ–‡ä»¶
  python fix_html_image_links.py
  
  # ä¿®å¤æŒ‡å®šäº¤æ˜“æ‰€
  python fix_html_image_links.py --exchange NASDAQ
  
  # ä¿®å¤æŠ½æ ·æ–‡ä»¶
  python fix_html_image_links.py --sample 50
        """
    )
    
    parser.add_argument(
        '--exchange',
        type=str,
        help='æŒ‡å®šäº¤æ˜“æ‰€ï¼ˆå¦‚ï¼šNASDAQ, NYSEï¼‰'
    )
    
    parser.add_argument(
        '--sample',
        type=int,
        help='éšæœºæŠ½æ ·çš„æ–‡ä»¶æ•°é‡'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶'
    )
    
    args = parser.parse_args()
    
    # è¿è¡Œä¿®å¤
    fixer = HTMLImageLinkFixer(dry_run=args.dry_run)
    fixer.run(
        exchange=args.exchange,
        sample_size=args.sample
    )
    
    return 0


if __name__ == '__main__':
    sys.exit(main())

