"""
å¯¼å‡ºæ–‡ä»¶å®Œæ•´æ€§æŠ¥å‘Šä¸ºMarkdownæ ¼å¼
Professional report generation with detailed analysis
"""

import os
import sys
from pathlib import Path
from collections import defaultdict
from datetime import datetime
from typing import Dict, List

from sqlalchemy import create_engine, text
from config.settings import settings


class IntegrityReportExporter:
    """å®Œæ•´æ€§æŠ¥å‘Šå¯¼å‡ºå™¨"""
    
    def __init__(self):
        self.storage_root = Path(settings.storage_root)
        self.engine = create_engine(settings.database_url)
        self.report_lines = []
        
        # ç»Ÿè®¡æ•°æ®
        self.file_stats = {
            'total_files': 0,
            'total_size': 0,
            'by_exchange': defaultdict(lambda: {'count': 0, 'size': 0}),
            'by_type': defaultdict(lambda: {'count': 0, 'size': 0}),
            'by_year': defaultdict(lambda: {'count': 0, 'size': 0}),
            'by_company': defaultdict(int)
        }
        
        self.db_records = {
            'total': 0,
            'downloaded': 0,
            'by_exchange': defaultdict(int),
            'by_type': defaultdict(int),
            'by_status': defaultdict(int)
        }
    
    def add_line(self, line: str = ""):
        """æ·»åŠ ä¸€è¡Œåˆ°æŠ¥å‘Š"""
        self.report_lines.append(line)
    
    def format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} PB"
    
    def scan_filesystem(self) -> Dict:
        """æ‰«ææ–‡ä»¶ç³»ç»Ÿ"""
        print("ğŸ“ æ‰«ææ–‡ä»¶ç³»ç»Ÿ...")
        
        if not self.storage_root.exists():
            print(f"âš ï¸  å­˜å‚¨ç›®å½•ä¸å­˜åœ¨: {self.storage_root}")
            return {'files': [], 'file_paths': set(), 'file_paths_by_type': defaultdict(set), 'file_paths_by_exchange_type': defaultdict(lambda: defaultdict(set))}
        
        all_files = []
        file_paths_set = set()
        file_paths_by_type = defaultdict(set)  # æŒ‰ç±»å‹åˆ†ç»„çš„è·¯å¾„
        file_paths_by_exchange_type = defaultdict(lambda: defaultdict(set))  # æŒ‰äº¤æ˜“æ‰€å’Œç±»å‹åˆ†ç»„
        
        for exchange_dir in self.storage_root.iterdir():
            if not exchange_dir.is_dir():
                continue
            
            exchange = exchange_dir.name
            
            for company_dir in exchange_dir.iterdir():
                if not company_dir.is_dir():
                    continue
                
                ticker = company_dir.name
                
                for year_dir in company_dir.iterdir():
                    if not year_dir.is_dir():
                        continue
                    
                    try:
                        year = int(year_dir.name)
                    except ValueError:
                        continue
                    
                    for file_path in year_dir.rglob('*'):
                        if file_path.is_file() and not file_path.name.startswith('.'):
                            try:
                                file_size = file_path.stat().st_size
                                
                                if file_path.parent.name == 'xbrl':
                                    file_type = 'xbrl'
                                elif file_path.suffix.lower() in ['.html', '.htm']:
                                    file_type = 'html'
                                elif file_path.suffix.lower() in ['.png', '.jpg', '.jpeg', '.gif']:
                                    file_type = 'image'
                                else:
                                    file_type = 'other'
                                
                                file_info = {
                                    'path': file_path,
                                    'relative_path': file_path.relative_to(self.storage_root),
                                    'exchange': exchange,
                                    'ticker': ticker,
                                    'year': year,
                                    'type': file_type,
                                    'size': file_size,
                                    'name': file_path.name
                                }
                                
                                all_files.append(file_info)
                                relative_path = str(file_path.relative_to(self.storage_root))
                                file_paths_set.add(relative_path)
                                file_paths_by_type[file_type].add(relative_path)
                                file_paths_by_exchange_type[exchange][file_type].add(relative_path)
                                
                                self.file_stats['total_files'] += 1
                                self.file_stats['total_size'] += file_size
                                self.file_stats['by_exchange'][exchange]['count'] += 1
                                self.file_stats['by_exchange'][exchange]['size'] += file_size
                                self.file_stats['by_type'][file_type]['count'] += 1
                                self.file_stats['by_type'][file_type]['size'] += file_size
                                self.file_stats['by_year'][year]['count'] += 1
                                self.file_stats['by_year'][year]['size'] += file_size
                                self.file_stats['by_company'][f"{exchange}/{ticker}"] += 1
                                
                            except Exception as e:
                                pass
        
        print(f"âœ… æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
        return {
            'files': all_files,
            'file_paths': file_paths_set,
            'file_paths_by_type': file_paths_by_type,
            'file_paths_by_exchange_type': file_paths_by_exchange_type
        }
    
    def query_database_records(self) -> Dict:
        """æŸ¥è¯¢æ•°æ®åº“è®°å½•"""
        print("ğŸ—„ï¸  æŸ¥è¯¢æ•°æ®åº“...")
        
        with self.engine.connect() as conn:
            result = conn.execute(text("""
                SELECT 
                    a.id,
                    a.local_path,
                    a.artifact_type,
                    a.status,
                    a.file_size,
                    c.exchange,
                    c.ticker,
                    f.fiscal_year
                FROM artifacts a
                JOIN filings f ON a.filing_id = f.id
                JOIN companies c ON f.company_id = c.id
                WHERE a.status IN ('downloaded', 'skipped')
                  AND a.local_path IS NOT NULL;
            """))
            
            db_files = []
            db_paths_set = set()
            db_paths_by_type = defaultdict(set)  # æŒ‰ç±»å‹åˆ†ç»„çš„è·¯å¾„
            db_paths_by_exchange_type = defaultdict(lambda: defaultdict(set))  # æŒ‰äº¤æ˜“æ‰€å’Œç±»å‹åˆ†ç»„
            
            for row in result:
                artifact_id, local_path, artifact_type, status, file_size, exchange, ticker, year = row
                
                db_files.append({
                    'id': artifact_id,
                    'local_path': local_path,
                    'type': artifact_type,
                    'status': status,
                    'size': file_size,
                    'exchange': exchange,
                    'ticker': ticker,
                    'year': year
                })
                
                if local_path:
                    db_paths_set.add(local_path)
                    db_paths_by_type[artifact_type].add(local_path)
                    db_paths_by_exchange_type[exchange][artifact_type].add(local_path)
                
                self.db_records['total'] += 1
                if status == 'downloaded':
                    self.db_records['downloaded'] += 1
                self.db_records['by_exchange'][exchange] += 1
                self.db_records['by_type'][artifact_type] += 1
                self.db_records['by_status'][status] += 1
            
            print(f"âœ… æŸ¥è¯¢åˆ° {len(db_files)} æ¡è®°å½•")
            return {
                'records': db_files,
                'db_paths': db_paths_set,
                'db_paths_by_type': db_paths_by_type,
                'db_paths_by_exchange_type': db_paths_by_exchange_type
            }
    
    def generate_markdown_report(self, fs_data: Dict, db_data: Dict):
        """ç”ŸæˆMarkdownæŠ¥å‘Š"""
        print("ğŸ“ ç”ŸæˆMarkdownæŠ¥å‘Š...")
        
        fs_paths = fs_data.get('file_paths', set())
        db_paths = db_data.get('db_paths', set())
        
        missing_in_fs = db_paths - fs_paths
        extra_in_fs = fs_paths - db_paths
        matched = fs_paths & db_paths
        
        # å¼€å§‹ç”ŸæˆæŠ¥å‘Š
        self.add_line("# ğŸ“Š SECæŠ¥å‘Šæ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š")
        self.add_line()
        self.add_line(f"**ç”Ÿæˆæ—¶é—´ï¼š** {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        self.add_line(f"**å­˜å‚¨ä½ç½®ï¼š** `{self.storage_root}`")
        self.add_line(f"**æ£€æŸ¥èŒƒå›´ï¼š** 2023-2025å¹´ NASDAQ & NYSE ä¸Šå¸‚å…¬å¸å¹´æŠ¥/å­£æŠ¥")
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # æ‰§è¡Œæ‘˜è¦
        self.add_line("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦")
        self.add_line()
        
        match_rate = (len(matched) / len(db_paths) * 100) if db_paths else 0
        score = self._calculate_score(len(matched), len(db_paths), len(missing_in_fs), len(extra_in_fs), len(fs_paths))
        
        # è®¡ç®—æŒ‰ç±»å‹çš„åŒ¹é…ç‡ï¼ˆç”¨äºæ‰§è¡Œæ‘˜è¦ï¼‰
        fs_paths_by_type = fs_data.get('file_paths_by_type', {})
        db_paths_by_type = db_data.get('db_paths_by_type', {})
        
        html_fs = fs_paths_by_type.get('html', set())
        html_db = db_paths_by_type.get('html', set())
        html_matched = html_fs & html_db
        html_match_rate = (len(html_matched) / len(html_db) * 100) if len(html_db) > 0 else 0
        
        image_fs = fs_paths_by_type.get('image', set())
        image_db = db_paths_by_type.get('image', set())
        image_matched = image_fs & image_db
        image_match_rate = (len(image_matched) / len(image_db) * 100) if len(image_db) > 0 else 0
        
        self.add_line("| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |")
        self.add_line("|------|------|------|")
        self.add_line(f"| **å®Œæ•´æ€§è¯„åˆ†** | **{score:.1f}/100** | {self._get_rating(score)} |")
        self.add_line(f"| å®é™…æ–‡ä»¶æ€»æ•° | {self.file_stats['total_files']:,} | æœ¬åœ°å­˜å‚¨çš„æ–‡ä»¶æ•°é‡ |")
        self.add_line(f"| æ–‡ä»¶æ€»å¤§å° | {self.format_size(self.file_stats['total_size'])} | å®é™…å ç”¨å­˜å‚¨ç©ºé—´ |")
        self.add_line(f"| æ•°æ®åº“è®°å½•æ•° | {self.db_records['total']:,} | åº”ä¸‹è½½çš„æ–‡ä»¶è®°å½• |")
        self.add_line(f"| **æ€»ä½“åŒ¹é…ç‡** | **{match_rate:.2f}%** | æ–‡ä»¶ç³»ç»Ÿä¸æ•°æ®åº“ä¸€è‡´æ€§ |")
        self.add_line(f"| â””â”€ HTMLåŒ¹é…ç‡ | {html_match_rate:.2f}% | HTMLæ–‡ä»¶åŒ¹é…ç‡ ({len(html_matched):,}/{len(html_db):,}) |")
        self.add_line(f"| â””â”€ IMAGEåŒ¹é…ç‡ | {image_match_rate:.2f}% | å›¾ç‰‡æ–‡ä»¶åŒ¹é…ç‡ ({len(image_matched):,}/{len(image_db):,}) |")
        self.add_line(f"| è¦†ç›–å…¬å¸æ•° | {len(self.file_stats['by_company']):,} | æœ‰æ–‡ä»¶çš„å…¬å¸æ•°é‡ |")
        self.add_line()
        
        # æ•°æ®å®Œæ•´æ€§çŠ¶æ€
        if score >= 90:
            self.add_line("### âœ… æ•°æ®è´¨é‡è¯„ä¼°ï¼šä¼˜ç§€")
            self.add_line()
            self.add_line("æ•°æ®å®Œæ•´æ€§è‰¯å¥½ï¼Œæ–‡ä»¶åŒ¹é…ç‡é«˜ï¼Œå¯ä»¥æ”¾å¿ƒä½¿ç”¨è¿›è¡Œåˆ†æã€‚")
        elif score >= 80:
            self.add_line("### âš ï¸ æ•°æ®è´¨é‡è¯„ä¼°ï¼šè‰¯å¥½")
            self.add_line()
            self.add_line("æ•°æ®å®Œæ•´æ€§è¾ƒå¥½ï¼Œå­˜åœ¨å°‘é‡é—®é¢˜ï¼Œå»ºè®®reviewåä½¿ç”¨ã€‚")
        else:
            self.add_line("### âŒ æ•°æ®è´¨é‡è¯„ä¼°ï¼šéœ€è¦æ”¹è¿›")
            self.add_line()
            self.add_line("æ•°æ®å®Œæ•´æ€§å­˜åœ¨è¾ƒå¤šé—®é¢˜ï¼Œå»ºè®®å…ˆä¿®å¤åå†ä½¿ç”¨ã€‚")
        
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # 1. æ–‡ä»¶ç³»ç»Ÿæ¦‚è§ˆ
        self.add_line("## ğŸ“ æ–‡ä»¶ç³»ç»Ÿæ¦‚è§ˆ")
        self.add_line()
        
        self.add_line("### æŒ‰äº¤æ˜“æ‰€ç»Ÿè®¡")
        self.add_line()
        self.add_line("| äº¤æ˜“æ‰€ | æ–‡ä»¶æ•° | æ€»å¤§å° | æ•°æ®åº“è®°å½• | å®Œæ•´ç‡ |")
        self.add_line("|--------|--------|--------|-----------|--------|")
        
        for exchange in sorted(self.file_stats['by_exchange'].keys()):
            fs_stats = self.file_stats['by_exchange'][exchange]
            db_count = self.db_records['by_exchange'].get(exchange, 0)
            completeness = (fs_stats['count'] / db_count * 100) if db_count > 0 else 0
            
            self.add_line(f"| {exchange} | {fs_stats['count']:,} | "
                         f"{self.format_size(fs_stats['size'])} | {db_count:,} | {completeness:.1f}% |")
        
        self.add_line()
        
        # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        self.add_line("### æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡")
        self.add_line()
        self.add_line("| æ–‡ä»¶ç±»å‹ | æ–‡ä»¶æ•° | æ€»å¤§å° | å æ¯”ï¼ˆæ•°é‡ï¼‰ | å æ¯”ï¼ˆå¤§å°ï¼‰ |")
        self.add_line("|---------|--------|--------|------------|------------|")
        
        total_count = self.file_stats['total_files']
        total_size = self.file_stats['total_size']
        
        for file_type in sorted(self.file_stats['by_type'].keys()):
            stats = self.file_stats['by_type'][file_type]
            count_pct = (stats['count'] / total_count * 100) if total_count > 0 else 0
            size_pct = (stats['size'] / total_size * 100) if total_size > 0 else 0
            
            self.add_line(f"| {file_type} | {stats['count']:,} | "
                         f"{self.format_size(stats['size'])} | {count_pct:.1f}% | {size_pct:.1f}% |")
        
        self.add_line()
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡
        self.add_line("### æŒ‰å¹´ä»½ç»Ÿè®¡")
        self.add_line()
        self.add_line("| å¹´ä»½ | æ–‡ä»¶æ•° | æ€»å¤§å° | å æ¯” |")
        self.add_line("|------|--------|--------|------|")
        
        for year in sorted(self.file_stats['by_year'].keys(), reverse=True):
            stats = self.file_stats['by_year'][year]
            pct = (stats['count'] / total_count * 100) if total_count > 0 else 0
            
            self.add_line(f"| {year} | {stats['count']:,} | "
                         f"{self.format_size(stats['size'])} | {pct:.1f}% |")
        
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # 2. æ•°æ®åº“è®°å½•åˆ†æ
        self.add_line("## ğŸ—„ï¸ æ•°æ®åº“è®°å½•åˆ†æ")
        self.add_line()
        
        self.add_line("### ä¸‹è½½çŠ¶æ€åˆ†å¸ƒ")
        self.add_line()
        self.add_line("| çŠ¶æ€ | æ•°é‡ | å æ¯” |")
        self.add_line("|------|------|------|")
        
        for status in sorted(self.db_records['by_status'].keys()):
            count = self.db_records['by_status'][status]
            pct = (count / self.db_records['total'] * 100) if self.db_records['total'] > 0 else 0
            
            status_icon = "âœ…" if status == 'downloaded' else "â­ï¸"
            self.add_line(f"| {status_icon} {status} | {count:,} | {pct:.2f}% |")
        
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # 3. æŒ‰ç±»å‹ç»Ÿè®¡çš„å®Œæ•´æ€§ï¼ˆç»Ÿä¸€å£å¾„ï¼‰
        self.add_line("## ğŸ“ æŒ‰ç±»å‹ç»Ÿè®¡çš„å®Œæ•´æ€§ï¼ˆç»Ÿä¸€å£å¾„ï¼‰")
        self.add_line()
        self.add_line("### å…¨å±€åŒ¹é…ç‡ï¼ˆæŒ‰ç±»å‹ï¼‰")
        self.add_line()
        
        fs_paths_by_type = fs_data.get('file_paths_by_type', {})
        db_paths_by_type = db_data.get('db_paths_by_type', {})
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„åŒ¹é…ç‡
        type_stats = {}
        for artifact_type in set(list(fs_paths_by_type.keys()) + list(db_paths_by_type.keys())):
            fs_paths_type = fs_paths_by_type.get(artifact_type, set())
            db_paths_type = db_paths_by_type.get(artifact_type, set())
            matched_type = fs_paths_type & db_paths_type
            
            type_stats[artifact_type] = {
                'db_count': len(db_paths_type),
                'fs_count': len(fs_paths_type),
                'matched': len(matched_type),
                'match_rate': (len(matched_type) / len(db_paths_type) * 100) if len(db_paths_type) > 0 else 0
            }
        
        self.add_line("| æ–‡ä»¶ç±»å‹ | æ•°æ®åº“è®°å½•æ•° | æ–‡ä»¶ç³»ç»Ÿæ–‡ä»¶æ•° | åŒ¹é…æ•° | åŒ¹é…ç‡ |")
        self.add_line("|---------|------------|--------------|--------|--------|")
        
        total_db_weighted = 0
        total_matched_weighted = 0
        
        for artifact_type in sorted(type_stats.keys()):
            stats = type_stats[artifact_type]
            self.add_line(f"| **{artifact_type}** | {stats['db_count']:,} | {stats['fs_count']:,} | "
                         f"{stats['matched']:,} | **{stats['match_rate']:.2f}%** |")
            total_db_weighted += stats['db_count']
            total_matched_weighted += stats['matched']
        
        # åŠ æƒæ€»åˆ†
        overall_match_rate = (total_matched_weighted / total_db_weighted * 100) if total_db_weighted > 0 else 0
        self.add_line(f"| **Overall (åŠ æƒ)** | {total_db_weighted:,} | {len(fs_paths):,} | "
                     f"{total_matched_weighted:,} | **{overall_match_rate:.2f}%** |")
        
        self.add_line()
        
        # æŒ‰äº¤æ˜“æ‰€å’Œç±»å‹ç»Ÿè®¡
        self.add_line("### æŒ‰äº¤æ˜“æ‰€å’Œç±»å‹ç»Ÿè®¡çš„å®Œæ•´ç‡")
        self.add_line()
        
        fs_by_exchange_type = fs_data.get('file_paths_by_exchange_type', {})
        db_by_exchange_type = db_data.get('db_paths_by_exchange_type', {})
        
        all_exchanges = sorted(set(list(fs_by_exchange_type.keys()) + list(db_by_exchange_type.keys())))
        
        for exchange in all_exchanges:
            self.add_line(f"#### {exchange}")
            self.add_line()
            self.add_line("| æ–‡ä»¶ç±»å‹ | æ•°æ®åº“è®°å½•æ•° | æ–‡ä»¶ç³»ç»Ÿæ–‡ä»¶æ•° | åŒ¹é…æ•° | åŒ¹é…ç‡ |")
            self.add_line("|---------|------------|--------------|--------|--------|")
            
            fs_exchange = fs_by_exchange_type.get(exchange, {})
            db_exchange = db_by_exchange_type.get(exchange, {})
            
            all_types = sorted(set(list(fs_exchange.keys()) + list(db_exchange.keys())))
            
            exchange_total_db = 0
            exchange_total_matched = 0
            
            for artifact_type in all_types:
                fs_paths_ex_type = fs_exchange.get(artifact_type, set())
                db_paths_ex_type = db_exchange.get(artifact_type, set())
                matched_ex_type = fs_paths_ex_type & db_paths_ex_type
                
                db_count = len(db_paths_ex_type)
                fs_count = len(fs_paths_ex_type)
                matched_count = len(matched_ex_type)
                match_rate = (matched_count / db_count * 100) if db_count > 0 else 0
                
                self.add_line(f"| {artifact_type} | {db_count:,} | {fs_count:,} | "
                             f"{matched_count:,} | {match_rate:.2f}% |")
                
                exchange_total_db += db_count
                exchange_total_matched += matched_count
            
            # äº¤æ˜“æ‰€åŠ æƒæ€»åˆ†
            exchange_overall = (exchange_total_matched / exchange_total_db * 100) if exchange_total_db > 0 else 0
            self.add_line(f"| **å°è®¡ (åŠ æƒ)** | {exchange_total_db:,} | - | "
                         f"{exchange_total_matched:,} | **{exchange_overall:.2f}%** |")
            self.add_line()
        
        self.add_line("---")
        self.add_line()
        
        # 4. å®Œæ•´æ€§åˆ†æ
        self.add_line("## ğŸ” å®Œæ•´æ€§åˆ†æï¼ˆæ€»ä½“ï¼‰")
        self.add_line()
        
        self.add_line(f"- **åŒ¹é…æ–‡ä»¶æ•°ï¼š** {len(matched):,}")
        self.add_line(f"- **åŒ¹é…ç‡ï¼š** {match_rate:.2f}%")
        self.add_line(f"- **ç¼ºå¤±æ–‡ä»¶æ•°ï¼š** {len(missing_in_fs):,}")
        self.add_line(f"- **å¤šä½™æ–‡ä»¶æ•°ï¼š** {len(extra_in_fs):,}")
        self.add_line()
        
        if missing_in_fs:
            self.add_line("### âš ï¸ ç¼ºå¤±æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰50ä¸ªï¼‰")
            self.add_line()
            self.add_line("ä»¥ä¸‹æ–‡ä»¶åœ¨æ•°æ®åº“ä¸­æœ‰è®°å½•ï¼Œä½†åœ¨æ–‡ä»¶ç³»ç»Ÿä¸­ç¼ºå¤±ï¼š")
            self.add_line()
            for i, path in enumerate(list(missing_in_fs)[:50], 1):
                self.add_line(f"{i}. `{path}`")
            
            if len(missing_in_fs) > 50:
                self.add_line(f"\n... è¿˜æœ‰ {len(missing_in_fs) - 50} ä¸ªæ–‡ä»¶æœªåˆ—å‡º")
            self.add_line()
        
        if extra_in_fs:
            self.add_line("### â„¹ï¸ å¤šä½™æ–‡ä»¶åˆ—è¡¨ï¼ˆå‰50ä¸ªï¼‰")
            self.add_line()
            self.add_line("ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œä½†æ•°æ®åº“ä¸­æ²¡æœ‰è®°å½•ï¼š")
            self.add_line()
            for i, path in enumerate(list(extra_in_fs)[:50], 1):
                self.add_line(f"{i}. `{path}`")
            
            if len(extra_in_fs) > 50:
                self.add_line(f"\n... è¿˜æœ‰ {len(extra_in_fs) - 50} ä¸ªæ–‡ä»¶æœªåˆ—å‡º")
            self.add_line()
        
        if not missing_in_fs and not extra_in_fs:
            self.add_line("### âœ… å®Œç¾åŒ¹é…")
            self.add_line()
            self.add_line("æ‰€æœ‰æ–‡ä»¶éƒ½å®Œç¾åŒ¹é…ï¼Œæ²¡æœ‰ç¼ºå¤±æˆ–å¤šä½™çš„æ–‡ä»¶ã€‚")
            self.add_line()
        
        self.add_line("---")
        self.add_line()
        
        # 4. å…¬å¸è¦†ç›–ç‡åˆ†æ
        self.add_line("## ğŸ¢ å…¬å¸è¦†ç›–ç‡åˆ†æ")
        self.add_line()
        
        self.add_line(f"**æ€»è¦†ç›–å…¬å¸æ•°ï¼š** {len(self.file_stats['by_company']):,} å®¶")
        self.add_line()
        
        self.add_line("### Top 30 å…¬å¸ï¼ˆæŒ‰æ–‡ä»¶æ•°æ’åºï¼‰")
        self.add_line()
        self.add_line("| æ’å | å…¬å¸ (äº¤æ˜“æ‰€/ä»£ç ) | æ–‡ä»¶æ•° |")
        self.add_line("|------|------------------|--------|")
        
        top_companies = sorted(
            self.file_stats['by_company'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:30]
        
        for rank, (company, count) in enumerate(top_companies, 1):
            self.add_line(f"| {rank} | `{company}` | {count:,} |")
        
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # 5. æ•°æ®è´¨é‡æŒ‡æ ‡
        self.add_line("## ğŸ“Š æ•°æ®è´¨é‡æŒ‡æ ‡")
        self.add_line()
        
        missing_rate = (len(missing_in_fs) / len(db_paths) * 100) if db_paths else 0
        extra_rate = (len(extra_in_fs) / len(fs_paths) * 100) if fs_paths else 0
        
        self.add_line("| æŒ‡æ ‡ | æ•°å€¼ | ç›®æ ‡ | çŠ¶æ€ |")
        self.add_line("|------|------|------|------|")
        self.add_line(f"| æ–‡ä»¶åŒ¹é…ç‡ | {match_rate:.2f}% | â‰¥99% | {self._status_icon(match_rate >= 99)} |")
        self.add_line(f"| ç¼ºå¤±ç‡ | {missing_rate:.2f}% | â‰¤1% | {self._status_icon(missing_rate <= 1)} |")
        self.add_line(f"| å¤šä½™æ–‡ä»¶ç‡ | {extra_rate:.2f}% | â‰¤1% | {self._status_icon(extra_rate <= 1)} |")
        
        avg_files_per_company = self.file_stats['total_files'] / len(self.file_stats['by_company']) if self.file_stats['by_company'] else 0
        self.add_line(f"| å¹³å‡æ–‡ä»¶æ•°/å…¬å¸ | {avg_files_per_company:.1f} | â‰¥15 | {self._status_icon(avg_files_per_company >= 15)} |")
        
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # 6. å»ºè®®å’Œåç»­è¡ŒåŠ¨
        self.add_line("## ğŸ’¡ å»ºè®®å’Œåç»­è¡ŒåŠ¨")
        self.add_line()
        
        if score >= 95:
            self.add_line("### âœ… æ•°æ®è´¨é‡ä¼˜ç§€")
            self.add_line()
            self.add_line("1. æ•°æ®å®Œæ•´æ€§éå¸¸å¥½ï¼Œå¯ä»¥ç›´æ¥ç”¨äºç”Ÿäº§åˆ†æ")
            self.add_line("2. å»ºè®®å®šæœŸè¿è¡Œå®Œæ•´æ€§æ£€æŸ¥ï¼Œç¡®ä¿æŒç»­è´¨é‡")
            self.add_line("3. è€ƒè™‘è®¾ç½®è‡ªåŠ¨åŒ–çš„æ•°æ®å¤‡ä»½æµç¨‹")
        elif score >= 80:
            self.add_line("### âš ï¸ éœ€è¦å…³æ³¨çš„é—®é¢˜")
            self.add_line()
            if missing_in_fs:
                self.add_line(f"1. æœ‰ {len(missing_in_fs)} ä¸ªæ–‡ä»¶ç¼ºå¤±ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
            if extra_in_fs:
                self.add_line(f"2. æœ‰ {len(extra_in_fs)} ä¸ªå¤šä½™æ–‡ä»¶ï¼Œå»ºè®®reviewå¹¶æ¸…ç†")
            self.add_line("3. å»ºè®®è¿è¡Œå¢é‡æ›´æ–°è¡¥é½ç¼ºå¤±æ•°æ®")
        else:
            self.add_line("### âŒ éœ€è¦ç«‹å³å¤„ç†")
            self.add_line()
            self.add_line("1. **ä¼˜å…ˆçº§1ï¼š** ä¿®å¤ç¼ºå¤±çš„æ–‡ä»¶")
            self.add_line("2. **ä¼˜å…ˆçº§2ï¼š** æ¸…ç†å¤šä½™çš„æ–‡ä»¶")
            self.add_line("3. **ä¼˜å…ˆçº§3ï¼š** éªŒè¯æ•°æ®åº“è®°å½•çš„å‡†ç¡®æ€§")
        
        self.add_line()
        
        self.add_line("### æ¨èå‘½ä»¤")
        self.add_line()
        self.add_line("```bash")
        self.add_line("# è¿è¡Œå¢é‡æ›´æ–°")
        self.add_line("python main.py incremental")
        self.add_line()
        self.add_line("# é‡æ–°æ£€æŸ¥å®Œæ•´æ€§")
        self.add_line("python export_integrity_report.py")
        self.add_line()
        self.add_line("# æŸ¥çœ‹æ•°æ®åº“çŠ¶æ€")
        self.add_line("python query_db_summary.py")
        self.add_line("```")
        self.add_line()
        self.add_line("---")
        self.add_line()
        
        # é™„å½•
        self.add_line("## ğŸ“ é™„å½•")
        self.add_line()
        
        self.add_line("### æŠ€æœ¯è§„æ ¼")
        self.add_line()
        self.add_line(f"- **å­˜å‚¨æ ¹ç›®å½•ï¼š** `{self.storage_root}`")
        self.add_line(f"- **æ•°æ®åº“ï¼š** PostgreSQL")
        self.add_line(f"- **æ‰«ææ—¶é—´ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.add_line(f"- **Pythonç‰ˆæœ¬ï¼š** {sys.version.split()[0]}")
        self.add_line()
        
        self.add_line("### è¯„åˆ†æ ‡å‡†")
        self.add_line()
        self.add_line("å®Œæ•´æ€§è¯„åˆ†è®¡ç®—æ–¹å¼ï¼š")
        self.add_line()
        self.add_line("- **æ–‡ä»¶åŒ¹é…ç‡ï¼ˆ50åˆ†ï¼‰ï¼š** åŒ¹é…æ–‡ä»¶æ•° / æ•°æ®åº“è®°å½•æ•° Ã— 50")
        self.add_line("- **ç¼ºå¤±ç‡å¾—åˆ†ï¼ˆ25åˆ†ï¼‰ï¼š** max(0, 25 - ç¼ºå¤±ç‡ Ã— 100)")
        self.add_line("- **å¤šä½™æ–‡ä»¶å¾—åˆ†ï¼ˆ25åˆ†ï¼‰ï¼š** max(0, 25 - å¤šä½™ç‡ Ã— 100)")
        self.add_line()
        
        self.add_line("### è¯„çº§è¯´æ˜")
        self.add_line()
        self.add_line("| åˆ†æ•°èŒƒå›´ | è¯„çº§ | è¯´æ˜ |")
        self.add_line("|---------|------|------|")
        self.add_line("| 90-100 | â­â­â­â­â­ ä¼˜ç§€ | æ•°æ®è´¨é‡æä½³ |")
        self.add_line("| 80-89 | â­â­â­â­ è‰¯å¥½ | æ•°æ®è´¨é‡è¾ƒå¥½ |")
        self.add_line("| 70-79 | â­â­â­ ä¸­ç­‰ | éœ€è¦å…³æ³¨ |")
        self.add_line("| 60-69 | â­â­ åŠæ ¼ | éœ€è¦æ”¹è¿› |")
        self.add_line("| <60 | â­ ä¸åŠæ ¼ | éœ€è¦ç«‹å³å¤„ç† |")
        self.add_line()
        
        self.add_line("---")
        self.add_line()
        self.add_line(f"*æŠ¥å‘Šç”Ÿæˆäº {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')} by æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥å·¥å…·*")
        
        print("âœ… MarkdownæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
    
    def _calculate_score(self, matched: int, total_db: int, missing: int, extra: int, total_fs: int) -> float:
        """è®¡ç®—å®Œæ•´æ€§è¯„åˆ†"""
        score = 0
        
        # åŒ¹é…ç‡ (50åˆ†)
        if total_db > 0:
            score += (matched / total_db) * 50
        
        # ç¼ºå¤±ç‡ (25åˆ†)
        if total_db > 0:
            missing_rate = missing / total_db
            score += max(0, 25 - missing_rate * 100)
        
        # å¤šä½™æ–‡ä»¶ç‡ (25åˆ†)
        if total_fs > 0:
            extra_rate = extra / total_fs
            score += max(0, 25 - extra_rate * 100)
        
        return score
    
    def _get_rating(self, score: float) -> str:
        """è·å–è¯„çº§"""
        if score >= 90:
            return "â­â­â­â­â­ ä¼˜ç§€"
        elif score >= 80:
            return "â­â­â­â­ è‰¯å¥½"
        elif score >= 70:
            return "â­â­â­ ä¸­ç­‰"
        elif score >= 60:
            return "â­â­ åŠæ ¼"
        else:
            return "â­ ä¸åŠæ ¼"
    
    def _status_icon(self, condition: bool) -> str:
        """çŠ¶æ€å›¾æ ‡"""
        return "âœ…" if condition else "âš ï¸"
    
    def save_report(self, output_file: str = None):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if output_file is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"integrity_report_{timestamp}.md"
        
        output_path = Path(output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(self.report_lines))
        
        print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path.absolute()}")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size:,} å­—èŠ‚")
        
        return output_path
    
    def run(self, output_file: str = None):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        print("\n" + "=" * 100)
        print("ğŸ“Š ç”ŸæˆMarkdownæ ¼å¼å®Œæ•´æ€§æŠ¥å‘Š")
        print("=" * 100 + "\n")
        
        try:
            # 1. æ‰«ææ–‡ä»¶ç³»ç»Ÿ
            fs_data = self.scan_filesystem()
            
            # 2. æŸ¥è¯¢æ•°æ®åº“
            db_data = self.query_database_records()
            
            # 3. ç”ŸæˆMarkdownæŠ¥å‘Š
            self.generate_markdown_report(fs_data, db_data)
            
            # 4. ä¿å­˜æŠ¥å‘Š
            report_path = self.save_report(output_file)
            
            print("\n" + "=" * 100)
            print("âœ… æŠ¥å‘Šç”ŸæˆæˆåŠŸï¼")
            print("=" * 100)
            
            return report_path
            
        except Exception as e:
            print(f"\nâŒ ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å¯¼å‡ºæ–‡ä»¶å®Œæ•´æ€§æŠ¥å‘Šä¸ºMarkdownæ ¼å¼')
    parser.add_argument('-o', '--output', type=str, help='è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤ï¼šintegrity_report_YYYYMMDD_HHMMSS.mdï¼‰')
    args = parser.parse_args()
    
    exporter = IntegrityReportExporter()
    report_path = exporter.run(args.output)
    
    if report_path:
        print(f"\nğŸ“„ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æŠ¥å‘Šï¼š")
        print(f"   cat {report_path}")
        print(f"   æˆ–åœ¨IDEä¸­æ‰“å¼€æŸ¥çœ‹")
    
    sys.exit(0 if report_path else 1)


if __name__ == '__main__':
    main()

